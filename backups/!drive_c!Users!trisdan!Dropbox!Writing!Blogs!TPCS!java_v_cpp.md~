In Spring Quarter of 2013, I took ECS 170: Intro to AI at UC Davis. The professor was Ian Davidson. He was a very poor communicator, and he's very industry-focused, which was both good and bad: good, because he spent the second half of the course doing machine learning instead of pissing away a whole quarter on 1960s-era game playing crap, but bad because he made us implement all the machine learning stuff in Java. 

Java's not such a bad language. It's sort of ridiculously sprawling and rambling, which its 'public static void' and 'protected abstract ArrayList<Integer> a = new ArrayList<Integer>();', and it lacks lots of modern features like vector and map literals, and the standard library data structures are really confusing to use because all of them have three ways to retrieve an item, with different return types and levels of thread safety, and none of them ever does exactly what you want. Its main weakness for AI is how low-level it is; since AI deals with such complicated concepts, it's easier for me (and probably most people) to at least prototype in a language like Python or Common Lisp that works at a higher level of abstraction. But Java is better than C++; I thoroughly confirmed this to myself the quarter I took AI, because I also took Compilers, and we implemented our compilers in C++. In both classes, the instructor provided us with a bunch of starter code that we had to hook our own components into. To do that, we had to read and understand the starter code.

The Java starter code was pretty easy to get into; you could typically always tell what method you wanted or what a particular instance variable was for, and it was nicely documented with Javadoc, so all the high-level information about a method's purpose, along with its type signature and other miscellaneous info, was right there in a beautifully formatted HTML page. With all that, whenever I needed to interact with one of the starter classes, it was always easy to figure out which methods or variables I needed, and they always worked pretty much as I expected.

By contrast, the C++ starter code was a nightmare. It had no documentation at all, except for a confusing PDF file that vaguely covered the architectural design of the abstract syntax tree our code was supposed to be working with. What comments it did have were mostly of the "x = x + 1 // Add 1 to x" variety. It mixed modern C++ features with old C features in the most horrendous way possible. The AST code was a massive class hierarchy auto-generated by some tool, with awful names like "node" and "tree\_node" and "class\_class" and "class\_class\_\_Class" that were easy to confuse and hard to interpret. ("class_class" was the class that represented a class declaration in the source code of the language we were compiling, Cool—don't even get me started on how terrible Cool was.) Massive class hierarchies are hard to work with just because of their size, but this one was reasonably well-designed, in the way that you could use inheritance to add methods without having to rewrite anything—just add the declaration to the base class, then implement the method in the derived classes, and you have a new power throughout your abstract syntax tree. So that wasn't too bad, and if that was the extent of things, my partner and I could have written the rest of the code in Java-style and been reasonably happy with it.

But the starter code predated STL, and no one had bothered to rewrite it, so parts of the implementation depended on this godawful handmade linked list that, from the look of it, had been written for a lower-division data structures class by someone who barely passed the prerequisite class. The thing threw naked NULLs at us out of every method, but the site where this caused a segfault was usually far away from where we'd gotten the NULL, so I had to track the nulls back several methods to the place where they first appeared, then figure out how they had come about. Since there was no documentation, you only knew when a method would return null by looking at the source code, and you had to understand all of the list's bizarre internal settings to know if something you'd done would make it throw a null at you. Segfaults were the order of the day; I had to stop every hour or so and delete the coredump files, because the space in my computer lab account was getting so filled up with coredumps..

The list was contained inside classes, as if it were supposed to be object-oriented, but it had no interface at all, so the only way to iterate over a list was to access its internal pointers and set up your own fake iterator. Since you couldn't access the template parameter value from outside the class, the only way to do this was to figure out from looking at the declaration what type the list was holding. The declarations were all buried underneath mountains of abstract syntax tree code, and when you found them, you were in for a nasty surprise: most of the lists had  template parameters which were not instances of the AST classes, but rather pointers to instances of the AST classes. So what the list actually held was a pointer to a pointer to some AST class, because the list node held a pointer to whatever type its template parameter was. Then some idiot had helpfully typedef'd all the pointers to AST classes, pointers to pointers of AST classes, and pointers to lists of pointers to pointers to AST classes, so that they had inane names like 'class_class_Class' and 'class_class_Classes' and 'Classes' and 'Classes_' and 'Classes_class__', and proceeded to use those names EVERYWHERE instead of writing even a single character of pointer-handling code. 

I cannot tell you how much I hate this practice. Whatever clarity you're getting about what a pointer is supposed to represent, you're losing the vital cue that it's a POINTER to something, not just a something. Working with pointers is already fairly difficult because of the layering of identities that goes on when you use them; the last thing we need is to layer yet another identity on top of that. It's not like object-oriented programming, where you can hide a pointer inside a class and have all the work with the pointer be internal; when you typedef a pointer, it's still a pointer, and to do anything with it, you have to work with pointers. You're just making people who interact with your code stop and think, "Oh yes, that's actually a pointer, so I can't just use it, I have to do pointer stuff with it", at best as soon as they use the cloaked pointer, and at worst after spending an hour tracking down a segfault. 

At first, my partner and I were using this godawful homemade linked list whenever we needed a linked list in our code; we naively trusted that our instructor had given us this code to help us and that using it was a good idea. Then I started to notice the crappiness of the (nonexistent) interface, but we already had some code using the homemade list that I didn't want to rewrite, and I wanted to try and stay uniform with the rest of the code, so I kept using it. Then at some point I did something rather complicated; I don't remember exactly what I was doing, but I think it involved storing a pointer to a pointer to something in one of these lists, so that internally we had a pointer to a pointer to a pointer. I started getting segfaults all over the place, but I didn't suspect this piece of code for a long time, because it had done fine through several other methods. I spent two hours adding <code>assert(p);</code> for every pointer <code>p</code> in the code up to that point, to make sure they never became null. They never did. I fired up GDB and started stepping through, looking at things, and I saw something mindboggling. What I had thought were pointers to pointers to pointers to something had somehow become pointers to pointers to pointers to nothing. All the pointers were dangling.

To this day, I have no idea how this happened. (In fact, I might post a question to Stack Overflow asking for an explanation on this; I don't care anymore except in an academic sense, though, because I've already decided that I don't want to work with C++ at all during my career, if there's any other option, and as Steve Yegge said, "As far as I'm concerned, there's always another option.") The starter code did not do any memory management; it allocated objects and then never freed them. None of the classes had destructors beyond the default. It was basically a gigantic memory leak. So it wasn't a bug with things being freed that were still in use, the usual cause of dangling pointers. At the time I assumed that the default destructors had somehow been called by mistake because of all the indirection going on here (an instance variable holding a pointer to a list holding a pointer to a list node holding a pointer to a pointer to a pointer to a class instance).

I was under quite a bit of stress to finish this large, complex project so I could work on other things, so I decided to just drop a bomb on it and rewrite all of our code using the STL list. There were still a few places where I had to use the awful homemade linked list to interact with the starter code, but all of my code used the STL list, and although this project was still a gigantic pain in the ass, using STL collections to excise that particular tumor lifted about 30% of the burden and kept me from suffering a nervous breakdown. The assertions I'd added on all the pointer accesses while trying to debug this problem also turned out to be a huge help, and from then on, no pointer was used for anything after being modified unless I had first asserted that it wasn't null. That way the program would vomit its guts out the first time we got an unexpected null pointer, and I'd know exactly where to point GDB in this massive hairball of code to figure out where the null had crept in.

Now we get to the punchline, the reason I blame C++ in particular for this experience, and why it made me decide not to use C++ anymore. Because if you point a finger at me and say "You're just stupid and don't know how to work with low-level code!", I won't disagree with you. Pointers are hard to work with. I went through a phase where I used pointers and pointer arithmetic for everything, like using <code>char*** s;</code> to declare a two-dimensional array of strings instead of what I would use today, <code>vector<vector <string> > > s;</code>, and it bit me every time. And if you say "You clearly didn't know the right discipline for working with low-level code, but now that you've discovered that you really can't ever trust a pointer and have to check them every time, you've learned something", I won't disagree with you there either. I started programming with Python in 2010 on Windows Vista, not with Fortran in 1965 on a VAX. I never learned C in school (I'm trying to rectify that now). My classes at school all used C++ or Java, and you can mostly program C++ like it's Java if you're careful and never have to deal with anyone else's code. So I never really learned to program low-level code well; I did do a semester of x86 assembly, but what we did in the entire semester was essentially equivalent to the first week of a class using C++, and the first day of a class using Python.

But the problems with this code didn't come from being too low-level; they came from mixing high and low levels in horrific new ways. Take the awful linked list that required you to grub around in its implementation to iterate over it; understanding this code took me far longer than it needed to, because the linked list was using the high-level template concept in combination with the low-level concept of typedef'd pointers to pointers. In Java, this couldn't happen, because Java doesn't have typedefs or pointers. You might have an awful linked list that breaks encapsulation and uses generics in a stupid way, but Java's references are much easier to understand than pointers, and can't be typedef'd, and can't point to each other; so nothing you can do in Java in this situation can increase the danger of null pointer exceptions or force you to look up how many layers of indirection you need to pass through. And Java also has a culture behind it that discourages people from writing classes that require the client to grub around in the implementation.

I'll argue that this also couldn't happen in C. There are no templates in C, so if I know what type of struct the pointer to the head of the list is pointing to, I know what kind of data I can expect from the list. And since C doesn't have classes, I never have to deal with pointers to lists of nodes with pointers to pointers to pointers to instances of a hierarchy with fifty classes in it, where I can never actually know what the type of the object being pointed to really is. In C and Java, the features interact to create a complete system. The interaction of the features is usually clear and well-defined. In C++, you can make features that were never designed to go together interact. I used to regard this as an asset. Now I regard it as a liability. Because working on this code made me realize that in real life, you have to use code written by other programmers, and not all programmers have good taste. Some of them think it's a good idea to typedef all their pointers with stupid names that supposedly tell you what the pointers are, but cover up the part of their identity you need to care about the most, which is that they are pointers. Some programmers think it's a good idea to write their own bug-ridden versions of simple things like linked lists and then not take the time to even write a decent interface. Some of them have never even heard of the idea of hiding a class's implementation behind a sensible interface.

I don't want to work in a language that makes this easy for them. I want to work in languages that constrain the mixing of high- and low-level features so that you can always predict what will happen when two features interact. Because I'll admit it: I'm not that good of a programmer. I am not smart enough to figure out why memory is being deallocated when I make a pointer to an instance of a crappy bug-ridden linked list that contains a pointer to a pointer to a pointer to some derived class of the AST root. As a not-very-smart programmer, I absolutely do not want to waste what little brain power I have worrying about this crap.

So I'm not very smart. But that doesn't mean that people who don't want to use C++ are all as dumb as me. Read *Coders at Work*. Surely there's someone in there you respect. Lots of them love C. Lots of them use Java or Lisp or Python. Most of them started out on a VAX or some similarly non-timesharing system, programming Fortran and assembler, sometime between 1950 and 1980. And to a person, not a single one of them enjoys using C++. Evidently none of them thought the ability to freely mix low-level and high-level code was important enough to deal with all the needless complexity that comes with it.
